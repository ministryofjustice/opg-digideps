version: 2.1

workflows:
  dependabot_app:
    when:
      and:
        - or:
            [
              {
                matches:
                  {
                    pattern: "^dependabot/composer.+$",
                    value: << pipeline.git.branch >>,
                  },
              },
              {
                matches:
                  {
                    pattern: "^dependabot/npm_and_yarn.+$",
                    value: << pipeline.git.branch >>,
                  },
              },
            ]
        - not: { equal: [main, << pipeline.git.branch >>] }
    jobs:
      - build:
          name: build pr
          ecr_scan_notify: "False"
          push_ecr: false

      - client-unit-test:
          name: client unit test
          requires: [build pr]

      - api-unit-tests:
          name: api unit test
          requires: [build pr]

      - slack/approval-notification:
          name: branch complete notification
          message: Your branch << pipeline.git.branch >> has completed
          channel: opg-digicop-builds
          requires: [api unit test, client unit test]

  dependabot_terraform:
    when:
      and:
        - matches:
            {
              pattern: "^dependabot/terraform.+$",
              value: << pipeline.git.branch >>,
            }
        - not: { equal: [main, << pipeline.git.branch >>] }
    jobs:
      - lint:
          name: lint terraform

      - slack/approval-notification:
          name: branch complete notification
          message: Your branch << pipeline.git.branch >> has completed
          channel: opg-digicop-builds
          requires: [lint terraform]

  pull_request:
    when:
      and:
        - not:
            {
              matches:
                {
                  pattern: "^dependabot/.+$",
                  value: << pipeline.git.branch >>,
                },
            }
        - not: { equal: [main, << pipeline.git.branch >>] }
    jobs:
      - build:
          name: build pr
          ecr_scan_notify: "False"

      - cancel_redundant_builds:
          name: cancel redundant builds

      - tfsec-scan:
          name: scan with tfsec

      - lint:
          name: lint terraform

      #      - go-unit-tests:
      #          name: go unit tests

      - terraform-command:
          name: plan shared-development
          requires: [cancel redundant builds]
          tf_tier: shared
          tf_workspace: development
          tf_command: plan

      - workspace-protection:
          name: protect branch workspace
          requires: [cancel redundant builds]

      - terraform-command:
          name: plan branch env
          requires: [protect branch workspace]
          tf_command: plan

      - terraform-command:
          name: initial terraform apply
          requires: [lint terraform, plan shared-development, plan branch env]
          tf_command: apply
          tf_initial_apply: true

      - lambda-unit-test:
          name: lambda unit test
          requires: [lint terraform]

      - client-unit-test:
          name: client unit test
          requires: [build pr]

      - api-unit-tests:
          name: api unit test
          requires: [build pr]

      - terraform-command:
          name: terraform apply
          requires: [initial terraform apply, build pr, lambda unit test]
          tf_command: apply

      - scale-services:
          name: scale up services for integration tests
          replicas: "6"
          requires: [terraform apply]

      #            - pa11y-ci:
      #                name: accessibility test
      #                requires: [ apply environment ]
      #                filters: { branches: { ignore: [ main ] } }

      - run-task:
          name: integration tests reset DB
          requires: [terraform apply]
          task_name: integration_test_v2
          override: "sh,./tests/Behat/reset-db.sh"
          timeout: 500

      - run-task:
          name: integration tests v2 reports 1
          requires:
            [
              integration tests reset DB,
              scale up services for integration tests,
            ]
          task_name: integration_test_v2
          override: "sh,./tests/Behat/run-tests-parallel.sh,--tags,@v2_reporting_1,--profile,v2-tests-browserkit"
          timeout: 1200

      - run-task:
          name: integration tests v2 reports 2
          requires:
            [
              integration tests reset DB,
              scale up services for integration tests,
            ]
          task_name: integration_test_v2
          override: "sh,./tests/Behat/run-tests-parallel.sh,--tags,@v2_reporting_2,--profile,v2-tests-browserkit"
          timeout: 1200

      - run-task:
          name: integration tests v2 admin
          requires:
            [
              integration tests reset DB,
              scale up services for integration tests,
            ]
          task_name: integration_test_v2
          override: "sh,./tests/Behat/run-tests-parallel.sh,--tags,@v2_admin,--profile,v2-tests-browserkit"
          timeout: 1200

      - run-task:
          name: integration tests sequential only
          requires:
            [
              integration tests reset DB,
              scale up services for integration tests,
            ]
          task_name: integration_test_v2
          override: "sh,./tests/Behat/run-tests.sh,--tags,@v2_sequential,--profile,v2-tests-browserkit"
          timeout: 1200

      - terraform-plan-all:
          name: plan all terraform environments
          requires:
            [protect branch workspace, scale up services for integration tests]
          tf_tier: environment

      - terraform-plan-all:
          name: plan all shared terraform environments
          requires:
            [protect branch workspace, scale up services for integration tests]
          tf_tier: shared

      - scale-services:
          name: scale down services for integration tests
          replicas: "1"
          requires:
            [
              integration tests v2 reports 1,
              integration tests v2 reports 2,
              integration tests v2 admin,
              integration tests sequential only,
            ]

      - slack/approval-notification:
          name: branch complete notification
          message: Your branch << pipeline.git.branch >> has completed
          channel: opg-digicop-builds
          requires:
            [
              integration tests v2 reports 1,
              integration tests v2 reports 2,
              integration tests v2 admin,
              integration tests sequential only,
              plan all terraform environments,
              plan all shared terraform environments,
              api unit test,
              client unit test,
            ]

  integration:
    jobs:
      - lambda-unit-test:
          name: lambda unit test
          filters: { branches: { only: [main] } }

      - build:
          name: build integration
          build_dev: true
          ecr_scan_notify: "True"
          filters: { branches: { only: [main] } }

      - client-unit-test:
          name: client unit test
          requires: [build integration]
          filters: { branches: { only: [main] } }

      - api-unit-tests:
          name: api unit test
          requires: [build integration]
          filters: { branches: { only: [main] } }

      - terraform-command:
          name: apply shared-development
          requires: [build integration, lambda unit test]
          filters: { branches: { only: [main] } }
          tf_tier: shared
          tf_workspace: development
          tf_command: apply

      # Notice, no 'reset' or 'restore' over development so we can keep test data
      - terraform-command:
          name: apply development
          requires: [apply shared-development, lambda unit test]
          tf_workspace: development
          filters: { branches: { only: [main] } }
          tf_command: apply

      - terraform-command:
          name: plan shared-preproduction
          requires: [build integration]
          filters: { branches: { only: [main] } }
          tf_tier: shared
          tf_workspace: preproduction
          tf_command: plan

      - terraform-command:
          name: apply shared-preproduction
          requires: [plan shared-preproduction, lambda unit test]
          filters: { branches: { only: [main] } }
          tf_tier: shared
          tf_workspace: preproduction
          tf_command: apply

      - terraform-command:
          name: apply integration
          requires: [build integration, apply shared-preproduction]
          filters: { branches: { only: [main] } }
          tf_workspace: integration
          tf_command: apply

      - scale-services:
          name: scale up services for integration tests
          replicas: "6"
          profile: digideps-ci-pre
          tf_workspace: integration
          requires: [apply integration]
          filters: { branches: { only: [main] } }

      - run-task:
          name: integration tests reset DB
          requires: [apply integration]
          filters: { branches: { only: [main] } }
          task_name: integration_test_v2
          tf_workspace: integration
          override: "sh,./tests/Behat/reset-db.sh"
          timeout: 500

      - run-task:
          name: integration tests v2 reports 1
          requires:
            [
              integration tests reset DB,
              scale up services for integration tests,
            ]
          filters: { branches: { only: [main] } }
          task_name: integration_test_v2
          tf_workspace: integration
          override: "sh,./tests/Behat/run-tests-parallel.sh,--tags,@v2_reporting_1"
          timeout: 1200

      - run-task:
          name: integration tests v2 reports 2
          requires:
            [
              integration tests reset DB,
              scale up services for integration tests,
            ]
          filters: { branches: { only: [main] } }
          task_name: integration_test_v2
          tf_workspace: integration
          override: "sh,./tests/Behat/run-tests-parallel.sh,--tags,@v2_reporting_2"
          timeout: 1200

      - run-task:
          name: integration tests v2 admin
          requires:
            [
              integration tests reset DB,
              scale up services for integration tests,
            ]
          filters: { branches: { only: [main] } }
          task_name: integration_test_v2
          tf_workspace: integration
          override: "sh,./tests/Behat/run-tests-parallel.sh,--tags,@v2_admin"
          timeout: 1200

      - run-task:
          name: integration tests sequential only
          requires:
            [
              integration tests reset DB,
              scale up services for integration tests,
            ]
          filters: { branches: { only: [main] } }
          task_name: integration_test_v2
          tf_workspace: integration
          override: "sh,./tests/Behat/run-tests.sh,--tags,@v2_sequential,--profile,v2-tests-browserkit"
          timeout: 1200

      - scale-services:
          name: scale down services for integration tests
          replicas: "1"
          profile: digideps-ci-pre
          tf_workspace: integration
          requires:
            [
              integration tests v2 reports 1,
              integration tests v2 reports 2,
              integration tests v2 admin,
              integration tests sequential only,
            ]
          filters: { branches: { only: [main] } }

      - terraform-command:
          name: plan preproduction
          requires: [apply shared-development]
          filters: { branches: { only: [main] } }
          tf_workspace: preproduction
          tf_command: plan

      - terraform-command:
          name: apply preproduction
          requires:
            [
              api unit test,
              client unit test,
              integration tests v2 reports 1,
              integration tests v2 reports 2,
              integration tests v2 admin,
              integration tests sequential only,
              apply shared-preproduction,
            ]
          filters: { branches: { only: [main] } }
          tf_workspace: preproduction
          tf_command: apply

      - terraform-command:
          name: plan production
          requires: [apply preproduction]
          filters: { branches: { only: [main] } }
          tf_workspace: production02
          tf_command: plan

      - terraform-command:
          name: plan shared-production
          requires: [apply preproduction]
          filters: { branches: { only: [main] } }
          tf_tier: shared
          tf_workspace: production
          tf_command: plan

      - slack/approval-notification:
          name: release approval notification
          message: "Production is ready for release and pending approval"
          requires: [plan shared-production, plan production]
          filters: { branches: { only: [main] } }

      - approve:
          name: approve release to production
          type: approval
          requires: [plan shared-production, plan production]
          filters: { branches: { only: [main] } }

      - terraform-command:
          name: apply shared-production
          requires: [approve release to production]
          filters: { branches: { only: [main] } }
          tf_tier: shared
          tf_workspace: production
          tf_command: apply

      - terraform-command:
          name: apply training
          requires: [apply shared-production]
          filters: { branches: { only: [main] } }
          tf_workspace: training
          tf_command: apply

      - terraform-command:
          name: apply production
          requires: [apply shared-production]
          filters: { branches: { only: [main] } }
          pact_tag: true
          tf_workspace: production02
          tf_command: apply

      - run-task:
          name: backup production
          requires: [apply production]
          filters: { branches: { only: [main] } }
          task_name: backup
          tf_workspace: production02
          timeout: 700

      - run-task:
          name: restore production to preproduction
          requires: [backup production]
          filters: { branches: { only: [main] } }
          task_name: restore_from_production
          tf_workspace: preproduction
          timeout: 700

  weekly_integration_run:
    triggers:
      - schedule:
          cron: "00 05 * * 0"
          filters: { branches: { only: [main] } }
    jobs:
      - lambda-unit-test:
          name: lambda unit test
          filters: { branches: { only: [main] } }
      - build:
          name: build integration
          filters: { branches: { only: [main] } }
      - terraform-command:
          name: apply integration
          requires: [build integration]
          filters: { branches: { only: [main] } }
          tf_workspace: integration
          tf_command: apply
      - scale-services:
          name: scale up services for integration tests
          replicas: "6"
          profile: digideps-ci-pre
          tf_workspace: integration
          requires: [apply integration]
          filters: { branches: { only: [main] } }
      - run-task:
          name: integration tests reset DB
          requires: [apply integration]
          filters: { branches: { only: [main] } }
          task_name: integration_test_v2
          tf_workspace: integration
          override: "sh,./tests/Behat/reset-db.sh"
          timeout: 500
      - run-task:
          name: integration tests v2 reports 1
          requires:
            [
              integration tests reset DB,
              scale up services for integration tests,
            ]
          filters: { branches: { only: [main] } }
          task_name: integration_test_v2
          tf_workspace: integration
          override: "sh,./tests/Behat/run-tests-parallel.sh,--tags,@v2_reporting_1"
          timeout: 1200
      - run-task:
          name: integration tests v2 reports 2
          requires:
            [
              integration tests reset DB,
              scale up services for integration tests,
            ]
          filters: { branches: { only: [main] } }
          task_name: integration_test_v2
          tf_workspace: integration
          override: "sh,./tests/Behat/run-tests-parallel.sh,--tags,@v2_reporting_2"
          timeout: 1200
      - run-task:
          name: integration tests v2 admin
          requires:
            [
              integration tests reset DB,
              scale up services for integration tests,
            ]
          filters: { branches: { only: [main] } }
          task_name: integration_test_v2
          tf_workspace: integration
          override: "sh,./tests/Behat/run-tests-parallel.sh,--tags,@v2_admin"
          timeout: 1200
      - run-task:
          name: integration tests sequential only
          requires:
            [
              integration tests reset DB,
              scale up services for integration tests,
            ]
          filters: { branches: { only: [main] } }
          task_name: integration_test_v2
          tf_workspace: integration
          override: "sh,./tests/Behat/run-tests.sh,--tags,@v2_sequential,--profile,v2-tests-browserkit"
          timeout: 1200
      - scale-services:
          name: scale down services for integration tests
          replicas: "1"
          profile: digideps-ci-pre
          tf_workspace: integration
          requires:
            [
              integration tests v2 reports 1,
              integration tests v2 reports 2,
              integration tests v2 admin,
              integration tests sequential only,
            ]
          filters: { branches: { only: [main] } }
      - client-unit-test:
          name: client unit test
          requires: [apply integration]
          filters: { branches: { only: [main] } }
      - api-unit-tests:
          name: api unit test
          requires: [apply integration]
          filters: { branches: { only: [main] } }
      - cross-browser-test:
          name: cross browser test
          requires: [apply integration]
          filters: { branches: { only: [main] } }

  scheduled_workspace_deletion:
    triggers:
      - schedule:
          cron: "0 8,20 * * 0-6" # twice a day at 8am and 8pm
          filters: { branches: { only: [main] } }
    jobs:
      - destroy-workspaces:
          name: destroy non protected workspaces
          filters: { branches: { only: [main] } }

orbs:
  aws-cli: circleci/aws-cli@3.1.3
  slack: circleci/slack@3.4.2
  codecov: codecov/codecov@1.1.1
  tfsec: mycodeself/tfsec@1.1.0
  dockerhub_helper:
    orbs:
      docker: circleci/docker@2.1.4
    commands:
      dockerhub_login:
        steps:
          - docker/install-docker-credential-helper
          - docker/check:
              docker-password: DOCKER_ACCESS_TOKEN
              docker-username: DOCKER_USER
  ecs_helper:
    commands:
      install:
        steps:
          - run:
              name: Install runner
              working_directory: ~/project/ecs_helper
              command: go install -mod vendor ./cmd/runner
          - run:
              name: Install stabilizer
              working_directory: ~/project/ecs_helper
              command: go install -mod vendor ./cmd/stabilizer
          - run:
              name: Install pact_tags
              working_directory: ~/project/ecs_helper
              command: go install -mod vendor ./cmd/pact_tags
      build:
        steps:
          - run:
              name: Build redeployer
              working_directory: ~/project/shared/go_redeployer
              command: GOARCH=amd64 GOOS=linux go build -o main ./main.go
  terraform:
    executors:
      terraform:
        docker:
          - image: cimg/go:1.16
            auth:
              username: $DOCKER_USER
              password: $DOCKER_ACCESS_TOKEN
        resource_class: small
        environment:
          TF_VERSION: 1.2.1
          TF_SHA256SUM: 8cf8eb7ed2d95a4213fbfd0459ab303f890e79220196d1c4aae9ecf22547302e
          TF_CLI_ARGS_plan: -input=false -lock=false
          TF_CLI_ARGS_apply: -input=false -auto-approve
          TF_CLI_ARGS_destroy: -input=false -auto-approve
          TF_CLI_ARGS_init: -input=false -upgrade=true -reconfigure
    commands:
      install:
        steps:
          - run:
              name: Download Terraform
              command: curl -sfSO https://releases.hashicorp.com/terraform/${TF_VERSION}/terraform_${TF_VERSION}_linux_amd64.zip
          - run:
              name: Add Terraform SHA256SUM
              command: echo "${TF_SHA256SUM} terraform_${TF_VERSION}_linux_amd64.zip" > SHA256SUMS
          - run:
              name: Check Terraform SHA256SUM
              command: sha256sum -c --status SHA256SUMS
          - run:
              name: Install Terraform
              command: sudo unzip terraform_${TF_VERSION}_linux_amd64.zip -d /bin
      install_workspace_manager:
        steps:
          - run:
              name: install workspace manager
              command: |
                wget https://github.com/TomTucka/terraform-workspace-manager/releases/download/v0.3.0/terraform-workspace-manager_Linux_x86_64.tar.gz -O $HOME/terraform-workspace-manager.tar.gz
                sudo tar -xvf $HOME/terraform-workspace-manager.tar.gz -C /usr/local/bin
                sudo chmod +x /usr/local/bin/terraform-workspace-manager

jobs:
  lint:
    executor: terraform/terraform
    resource_class: small
    steps:
      - checkout
      - terraform/install
      - run:
          name: check for added raw to templates
          command: |
            if [ `grep -r 'raw' ./* | grep -v 'assetSource' | wc -l` -gt 1 ]
            then
              echo "Additional instances of raw added in template directory."
              echo "This can make us vulnerable to cross site scripting attacks."
              echo "Please check and amend accordingly."
              exit 1
            else
              exit 0
            fi
          working_directory: ~/project/client/templates
      - run:
          name: terraform lint
          command: terraform fmt -diff -check -recursive
      - run:
          name: terraform validate
          command: |
            terraform init -backend=false
            terraform validate
          working_directory: ~/project/environment
      - run:
          name: terraform shared validate
          command: |
            terraform init -backend=false
            terraform validate
          working_directory: ~/project/shared

  tfsec-scan:
    executor: terraform/terraform
    steps:
      - checkout
      - run:
          name: install tfsec
          command: |
            URL=$(curl -s https://api.github.com/repos/aquasecurity/tfsec/releases/latest | grep "tfsec-linux-amd64\"$" | grep browser_download_url | cut -d '"' -f 4)
            wget $URL
            chmod +x tfsec-linux-amd64
            sudo mv tfsec-linux-amd64 /usr/local/bin/tfsec
      - run:
          name: run tfsec
          command: tfsec || true

  terraform-plan-all:
    executor: terraform/terraform
    resource_class: small
    parameters:
      tf_tier:
        description: tier to alter - shared or environment
        default: environment
        type: string
    working_directory: ~/project/<< parameters.tf_tier >>
    environment:
      TF_TIER: << parameters.tf_tier >>
      INIT_APPLY: "true"
    steps:
      - checkout:
          path: ~/project
      - terraform/install
      - ecs_helper/install
      - when:
          condition:
            and:
              - equal: [shared, << parameters.tf_tier >>]
          steps:
            - ecs_helper/build
      - run:
          name: Initialize
          command: terraform init
      - run:
          name: Set environment
          command: ~/project/.circleci/set_env.sh >> $BASH_ENV
      - when:
          condition:
            and:
              - equal: [environment, << parameters.tf_tier >>]
          steps:
            # We already run on branch on shared dev so no repetition here.
            - run:
                name: Run plan on integration
                command: |
                  export TF_WORKSPACE=integration
                  terraform plan
            - run:
                name: Run plan on training
                command: |
                  export TF_WORKSPACE=training
                  terraform plan
            - run:
                name: Run plan on production
                command: |
                  export TF_WORKSPACE=production02
                  terraform plan
      - run:
          name: Run plan on preproduction
          command: |
            export TF_WORKSPACE=preproduction
            terraform plan
      - when:
          condition:
            and:
              - equal: [shared, << parameters.tf_tier >>]
          steps:
            - run:
                name: Run plan on shared production
                command: |
                  export TF_WORKSPACE=production
                  terraform plan

  terraform-command:
    executor: terraform/terraform
    resource_class: small
    parameters:
      tf_workspace:
        description: terraform workspace
        type: string
        default: ""
      tf_tier:
        description: tier to alter - shared or environment
        default: environment
        type: string
      tf_command:
        description: terraform command
        type: string
      tf_initial_apply:
        description: whether this is doing the initial environment apply
        default: false
        type: boolean
      pact_tag:
        description: whether to add environment tag to pact broker
        default: false
        type: boolean
    environment:
      WORKSPACE: << parameters.tf_workspace >>
      INIT_APPLY: << parameters.tf_initial_apply >>
    working_directory: ~/project/<< parameters.tf_tier >>
    steps:
      - checkout:
          path: ~/project
      - terraform/install
      - ecs_helper/install
      - aws-cli/install
      - when:
          condition:
            and:
              - equal: [shared, << parameters.tf_tier >>]
          steps:
            - ecs_helper/build
      - attach_workspace: { at: ~/project }
      - run:
          name: Initialize
          command: terraform init
      - run:
          name: Set environment
          command: ~/project/.circleci/set_env.sh >> $BASH_ENV
      - when:
          condition:
            and:
              - equal: [apply, << parameters.tf_command >>]
              - equal: [false, << parameters.tf_initial_apply >>]
          steps:
            - run:
                name: Extract lambda layers
                command: tar xvf ~/project/lambda_functions/layer.tar
                working_directory: ~/project/lambda_functions
      - when:
          condition:
            and:
              - equal: [true, << parameters.tf_initial_apply >>]
          steps:
            - run:
                name: Run << parameters.tf_command >>
                command: |
                  if [ $(terraform output | grep "No outputs found" | wc -l) -eq "0" ]
                  then
                    echo "Skipping apply as already exists"
                  else
                    terraform << parameters.tf_command >>
                  fi
      - when:
          condition:
            and:
              - equal: [false, << parameters.tf_initial_apply >>]
          steps:
            - run:
                name: Run << parameters.tf_command >>
                command: terraform << parameters.tf_command >>
      - when:
          condition: << parameters.pact_tag >>
          steps:
            - run:
                name: set pact environment variables
                command: ~/project/.circleci/set_env_pact.sh >> $BASH_ENV
            - run:
                name: tag pact commit with v<x>_production
                command: pact_tags

  workspace-protection:
    executor: terraform/terraform
    resource_class: small
    working_directory: ~/project/environment
    parameters:
      protect_time:
        description: time to protect workspace
        type: string
        default: "3"
    environment:
      PROTECT_TIME: << parameters.protect_time >>
    steps:
      - checkout:
          path: ~/project
      - terraform/install
      - terraform/install_workspace_manager
      - run:
          name: Set environment
          command: ~/project/.circleci/set_env.sh >> $BASH_ENV
      - run:
          name: Add workspace to protected list
          command: terraform-workspace-manager -register-workspace="${TF_WORKSPACE}" -time-to-protect="${PROTECT_TIME}" -aws-account-id=248804316466 -aws-iam-role=digideps-ci

  scale-services:
    docker:
      - image: cimg/python:3.10.7
        auth:
          username: $DOCKER_USER
          password: $DOCKER_ACCESS_TOKEN
    resource_class: small
    working_directory: ~/project/environment
    parameters:
      replicas:
        description: number of replicas
        type: string
        default: "1"
      profile:
        description: profile to use
        type: string
        default: "digideps-ci-dev"
      tf_workspace:
        description: terraform workspace
        type: string
        default: ""
    environment:
      AWS_CONFIG_FILE: ~/project/aws_config
      WORKSPACE: << parameters.tf_workspace >>
    steps:
      - checkout:
          path: ~/project
      - aws-cli/install
      - run:
          name: Set environment
          command: ~/project/.circleci/set_env.sh >> $BASH_ENV
      - run:
          name: scale services
          command: |
            aws ecs update-service --service front-${TF_WORKSPACE} --cluster ${TF_WORKSPACE} --desired-count << parameters.replicas >> --profile << parameters.profile >>
            aws ecs update-service --service admin-${TF_WORKSPACE} --cluster ${TF_WORKSPACE} --desired-count << parameters.replicas >> --profile << parameters.profile >>
            aws ecs update-service --service api-${TF_WORKSPACE} --cluster ${TF_WORKSPACE} --desired-count << parameters.replicas >> --profile << parameters.profile >>

  destroy-workspaces:
    executor: terraform/terraform
    resource_class: small
    working_directory: ~/project/environment
    steps:
      - checkout:
          path: ~/project
      - terraform/install
      - terraform/install_workspace_manager
      - attach_workspace: { at: ~/project }
      - run:
          name: initialise terraform
          command: terraform init
      - run:
          name: destroy unprotected workspaces
          command: |
            unset TF_WORKSPACE
            ./scripts/workspace_cleanup.sh $(terraform-workspace-manager -protected-workspaces=true -aws-account-id=248804316466 -aws-iam-role=digideps-ci)
      - slack/status:
          channel: opg-digideps-devs
          failure_message: scheduled destroy workspaces has failed.
          success_message: scheduled destroy workspaces has succeeded.

  cancel_redundant_builds:
    docker:
      - image: cimg/python:3.10.7
        auth:
          username: $DOCKER_USER
          password: $DOCKER_ACCESS_TOKEN
    resource_class: small
    steps:
      - checkout
      - run:
          name: install packages
          command: python3 -m pip install -r requirements.txt
          working_directory: ~/project/ecs_helper/cmd/circle_builds
      - run:
          name: Cancel running builds
          command: |
            python cancel_redundant_builds.py \
            --circle_project_username="${CIRCLE_PROJECT_USERNAME}" \
            --circle_project_reponame="${CIRCLE_PROJECT_REPONAME}" \
            --circle_branch="${CIRCLE_BRANCH}" \
            --circle_builds_token="${CIRCLE_BUILDS_TOKEN}" \
            --terms_to_waitfor="apply,plan,terraform" \
            --prod_job_terms=" production,shared-production"
          working_directory: ~/project/ecs_helper/cmd/circle_builds

  build:
    docker:
      - image: cimg/python:3.10.7
        auth:
          username: $DOCKER_USER
          password: $DOCKER_ACCESS_TOKEN
    resource_class: medium
    parameters:
      build_dev:
        description: whether to build the development client image
        type: boolean
        default: false
      ecr_scan_notify:
        description: whether to notify slack
        type: string
        default: "False"
      push_ecr:
        description: whether to push images to ECR
        type: boolean
        default: true
    environment:
      AWS_REGION: eu-west-1
      AWS_CONFIG_FILE: ~/project/aws_config
      AWS_REGISTRY: 311462405659.dkr.ecr.eu-west-1.amazonaws.com
      ECR_SCAN_NOTIFY: << parameters.ecr_scan_notify >>
    steps:
      - dockerhub_helper/dockerhub_login
      - setup_remote_docker:
          version: 20.10.17
      - aws-cli/install
      - add_ssh_keys:
          fingerprints:
            - 6f:4b:55:76:0e:cd:27:7d:ad:c3:28:38:53:69:5c:32
      - checkout
      - run:
          name: Set environment
          command: ~/project/.circleci/set_env.sh >> $BASH_ENV
      - run:
          name: Download common passwords file
          command: wget -O commonpasswords.txt "https://www.ncsc.gov.uk/static-assets/documents/PwnedPasswordsTop100k.txt"
          working_directory: ~/project/client/extra
      - run:
          name: Set version
          command: |
            export VERSION=${TF_WORKSPACE}-${CIRCLE_SHA1:0:7}
            export DEV_VERSION=development-${CIRCLE_SHA1:0:7}
            echo "export VERSION=${VERSION}" >> $BASH_ENV
            echo "export DEV_VERSION=${DEV_VERSION}" >> $BASH_ENV
            echo "$VERSION" >> ~/project/VERSION
      - persist_to_workspace:
          root: .
          paths:
            - VERSION
      - run:
          name: Show version
          command: echo ${VERSION}
      - run:
          name: Docker login
          command: aws ecr get-login-password --region $AWS_REGION --profile digideps-ci | docker login --username AWS --password-stdin $AWS_REGISTRY
      - run:
          name: Build images
          command: docker-compose -f docker-compose.ci.yml build --parallel
      - run:
          name: List API packages
          command: docker-compose -f docker-compose.ci.yml run --rm api apk list | sort
      - run:
          name: List client packages
          command: docker-compose -f docker-compose.ci.yml run --rm client apk list | sort
      - run:
          name: Check updated PHP files for errors
          command: |
            MERGE_BASE_COMMIT=( $(git merge-base main HEAD) )
            API_CHANGED_FILES=( $(git diff --relative=api --name-only --diff-filter=d $MERGE_BASE_COMMIT | grep .php) ) || [[ $? == 1 ]]
            CLIENT_CHANGED_FILES=( $(git diff --relative=client --name-only --diff-filter=d $MERGE_BASE_COMMIT | grep .php) ) || [[ $? == 1 ]]

            if [ -n "$API_CHANGED_FILES" ]; then
                docker-compose -f docker-compose.ci.yml run --rm api php bin/phpstan analyse $API_CHANGED_FILES --memory-limit=0 --level=max || echo "phpstan failed"
            fi

            if [ -n "$CLIENT_CHANGED_FILES" ]; then
                docker-compose -f docker-compose.ci.yml run --rm client php bin/phpstan analyse $CLIENT_CHANGED_FILES --memory-limit=0 --level=max || echo "phpstan failed"
            fi
      - run:
          name: Archive docker images
          command: |
            docker tag ${AWS_REGISTRY}/digideps/client:${VERSION} client:latest
            docker tag ${AWS_REGISTRY}/digideps/api:${VERSION} api:latest
            docker tag ${AWS_REGISTRY}/digideps/htmltopdf:${VERSION} htmltopdf:latest
            docker save -o client.tar client:latest
            docker save -o api.tar api:latest
            docker save -o htmltopdf.tar htmltopdf:latest
          background: << parameters.push_ecr >>
      - when:
          condition: << parameters.push_ecr >>
          steps:
            - run:
                name: Push images
                command: docker-compose -f docker-compose.ci.yml push
            - when:
                condition: << parameters.build_dev >>
                steps:
                  - run:
                      name: add robots.txt to frontend development
                      command: docker-compose -f docker-compose.ci.dev.yml build --build-arg AWS_REGISTRY=${AWS_REGISTRY} --build-arg VERSION=${VERSION}
                  - run:
                      name: Push image
                      command: docker-compose -f docker-compose.ci.dev.yml push
            - run:
                name: Install ECR Scan Results Requirements
                working_directory: ~/project/ecs_helper/check_ecr_scan_results
                command: |
                  pip install -r requirements.txt
            - run:
                name: Check ECR Scan Results
                working_directory: ~/project/ecs_helper/check_ecr_scan_results
                command: |
                  python3 aws_ecr_scan_results.py --search digideps --tag "${VERSION}" --slack_webhook "${SLACK_WEBHOOK}" --post_to_slack "${ECR_SCAN_NOTIFY}"
      - persist_to_workspace:
          root: .
          paths:
            - ./client.tar
            - ./api.tar
            - ./htmltopdf.tar

  lambda-unit-test:
    docker:
      - image: cimg/python:3.7.7
        auth:
          username: $DOCKER_USER
          password: $DOCKER_ACCESS_TOKEN
    resource_class: small
    steps:
      - dockerhub_helper/dockerhub_login
      - setup_remote_docker:
          version: 20.10.17
      - checkout
      - attach_workspace: { at: ~/project }
      - run:
          name: Install lambda requirements for unit tests
          working_directory: ~/project/lambda_functions
          command: |
            docker-compose up -d
            sleep 5
            docker-compose run pythontests python -m pytest
            docker-compose down
      - run:
          name: install requirements for all lambda layers
          command: |
            export LAYER_PATH=lambda_functions/layers/monitoring/python/lib/python3.7/site-packages
            python3 -m pip install -r lambda_functions/requirements/requirements.txt --target ./$LAYER_PATH/
            cd lambda_functions
            tar cvf layer.tar layers
      - persist_to_workspace:
          root: .
          paths:
            - ./lambda_functions/layer.tar

  # Issue in circle with mock server. Fix in github actions
  go-unit-tests:
    docker:
      - image: cimg/go:1.19.3
        auth:
          username: $DOCKER_USER
          password: $DOCKER_ACCESS_TOKEN
    resource_class: small
    steps:
      - checkout
      - run:
          name: run go tests
          command: go test
          working_directory: ~/project/file_scanning

  client-unit-test:
    docker:
      - image: cimg/python:3.10.7
        auth:
          username: $DOCKER_USER
          password: $DOCKER_ACCESS_TOKEN
    resource_class: small
    steps:
      - dockerhub_helper/dockerhub_login
      - setup_remote_docker:
          version: 20.10.17
      - checkout
      - attach_workspace: { at: ~/project }
      - run:
          name: load docker image
          command: |
            docker load -i ~/project/client.tar
            docker load -i ~/project/api.tar
            docker load -i ~/project/htmltopdf.tar
            docker tag client:latest admin:latest
            docker tag client:latest frontend:latest
      - aws-cli/install
      - run:
          name: Set environment
          command: |
            ~/project/.circleci/set_env.sh >> $BASH_ENV
            ~/project/.circleci/set_env_pact.sh >> $BASH_ENV
      - run:
          name: Run tests
          command: |
            APP_ENV=dev APP_DEBUG=0 docker-compose -f docker-compose.yml -f docker-compose.ci.test.yml --project-name client-unit-tests up -d --no-deps frontend pact-mock
            sleep 3

            docker-compose -f docker-compose.yml -f docker-compose.ci.test.yml --project-name client-unit-tests exec frontend sh scripts/client-unit-tests.sh
            docker-compose -f docker-compose.yml -f docker-compose.ci.test.yml --project-name client-unit-tests exec frontend chmod -R 777 tests/phpunit/coverage/client-unit-tests.xml
            docker cp "client-unit-tests-frontend:/var/www/tests/phpunit/coverage/client-unit-tests.xml" "./client-unit-tests.xml"

            docker-compose -f docker-compose.yml -f docker-compose.ci.test.yml --project-name client-unit-tests exec pact-mock cat /tmp/pacts/complete_the_deputy_report-opg_data.json > pact.json
            docker-compose -f docker-compose.yml -f docker-compose.ci.test.yml --project-name client-unit-tests stop pact-mock
      - codecov/upload:
          file: ./client-unit-tests.xml
          flags: client
      - store_artifacts:
          path: pact.json
          destination: Pact file

  api-unit-tests:
    docker:
      - image: cimg/php:8.0.14
        auth:
          username: $DOCKER_USER
          password: $DOCKER_ACCESS_TOKEN
    resource_class: small
    steps:
      - dockerhub_helper/dockerhub_login
      - setup_remote_docker:
          version: 20.10.17
      - checkout
      - attach_workspace: { at: ~/project }
      - run:
          name: load docker image
          command: |
            docker load -i ~/project/client.tar
            docker load -i ~/project/api.tar
            docker load -i ~/project/htmltopdf.tar
            docker tag client:latest admin:latest
            docker tag client:latest frontend:latest
      - run:
          name: Set environment
          command: |
            ~/project/.circleci/set_env.sh >> $BASH_ENV
      - run:
          name: Run tests
          command: |
            docker-compose -f docker-compose.yml -f docker-compose.ci.test.yml --project-name api-unit-tests up -d postgres
            docker-compose -f docker-compose.yml -f docker-compose.ci.test.yml --project-name api-unit-tests run api sh scripts/reset_db_structure_local.sh
            APP_ENV=test APP_DEBUG=0 docker-compose -f docker-compose.yml -f docker-compose.ci.test.yml --project-name api-unit-tests up -d api
            docker-compose -f docker-compose.yml -f docker-compose.ci.test.yml --project-name api-unit-tests exec api chmod -R 777 var
            docker-compose -f docker-compose.yml -f docker-compose.ci.test.yml --project-name api-unit-tests run --rm wait-for-it -address localstack:4566 --timeout=25 -debug
            docker-compose -f docker-compose.yml -f docker-compose.ci.test.yml --project-name api-unit-tests exec api sh scripts/apiunittest.sh

            docker-compose -f docker-compose.yml -f docker-compose.ci.test.yml --project-name api-unit-tests exec api chmod -R 777 tests/coverage/api-unit-tests.xml
            docker cp "api-unit-tests-api:/var/www/tests/coverage/api-unit-tests.xml" "./api-unit-tests.xml"
      - codecov/upload:
          file: ./api-unit-tests.xml
          flags: api

  cross-browser-test:
    machine:
      image: ubuntu-2004:202201-02
    steps:
      - dockerhub_helper/dockerhub_login
      - aws-cli/install
      - checkout
      - attach_workspace: { at: ~/project }
      - run:
          name: Set environment
          command: |
            ~/project/.circleci/set_env.sh >> $BASH_ENV
      - run:
          name: Get secrets
          command: |
            chmod 755 ecs_helper/cmd/get_secrets/get_secrets.sh
            source ./ecs_helper/cmd/get_secrets/get_secrets.sh
            touch .env behat/.env
            echo "export BROWSERSTACK_USERNAME=$BROWSERSTACK_USERNAME" >> $BASH_ENV
            echo "export BROWSERSTACK_KEY=$BROWSERSTACK_KEY" >> $BASH_ENV
      #Initial steps to reset database/fixtures and remove any persistent data
      - run:
          name: Run tests
          command: |
            docker-compose -f docker-compose.yml -f docker-compose.dev.yml -f docker-compose.browserstack.yml \
            --project-name cross-browser up -d --build --remove-orphans

            docker-compose -f docker-compose.yml -f docker-compose.dev.yml -f docker-compose.browserstack.yml \
            --project-name cross-browser rm -fsv postgres

            docker-compose -f docker-compose.yml -f docker-compose.dev.yml -f docker-compose.browserstack.yml \
            --project-name cross-browser up -d postgres

            docker-compose -f docker-compose.yml -f docker-compose.dev.yml -f docker-compose.browserstack.yml \
            --project-name cross-browser run --rm wait-for-it -address postgres:5432 --timeout=30 -debug

            docker-compose -f docker-compose.yml -f docker-compose.dev.yml -f docker-compose.browserstack.yml \
            --project-name cross-browser run --rm api sh scripts/reset_db_structure_local.sh

            docker-compose -f docker-compose.yml -f docker-compose.dev.yml -f docker-compose.browserstack.yml \
            --project-name cross-browser run --rm api sh scripts/reset_db_fixtures_local.sh


            ./behat/BrowserStackLocal --daemon "start" --key $BROWSERSTACK_KEY
            sudo ./generate_certs.sh
            echo "127.0.0.1 digideps.local admin.digideps.local api.digideps.local www.digideps.local" \
            | sudo tee -a /etc/hosts


            docker-compose -f docker-compose.yml -f docker-compose.dev.yml -f docker-compose.browserstack.yml \
            --project-name cross-browser run test --profile cross-browser-chrome --suite chrome

            docker-compose -f docker-compose.yml -f docker-compose.dev.yml -f docker-compose.browserstack.yml \
            --project-name cross-browser run test --profile cross-browser-ie11 --suite ie11

            docker-compose -f docker-compose.yml -f docker-compose.dev.yml -f docker-compose.browserstack.yml \
            --project-name cross-browser run test --profile cross-browser-android-chrome --suite android-chrome


            ./behat/BrowserStackLocal --daemon "stop" --key $BROWSERSTACK_KEY

  pa11y-ci:
    machine:
      image: ubuntu-2004:202111-02
    steps:
      - dockerhub_helper/dockerhub_login
      - checkout
      - attach_workspace: { at: ~/project }
      - run:
          name: Set environment
          command: |
            ~/project/.circleci/set_env.sh >> $BASH_ENV
      - run:
          name: Run pa11y
          command: |
            docker-compose down
            docker-compose -f docker-compose.yml up -d pa11y
            docker-compose exec frontend touch /var/www/.enableProdMode
            docker-compose exec admin touch /var/www/.enableProdMode
            docker-compose exec api touch /var/www/.enableProdMode
            docker-compose -f docker-compose.yml run --rm api sh scripts/reset_db_structure.sh
            docker-compose -f docker-compose.yml run --rm api sh scripts/reset_db_fixtures.sh
            sleep 10
            docker-compose -f docker-compose.yml run pa11y pa11y-ci || echo "Pa11y found some errors"

            # The || operator ensures that if pa11y exits with an error code, we echo an informative message
            # instead of failing the job and the pipeline too.

  run-task:
    executor: terraform/terraform
    resource_class: medium
    parameters:
      tf_workspace:
        description: terraform workspace
        type: string
        default: ""
      task_name:
        description: name of task to run
        type: string
      timeout:
        description: time the task will run for before timing out
        type: integer
        default: 120
      notify_slack:
        description: whether to notify specific task failure
        type: boolean
        default: false
      override:
        description: override value where task is overridable
        type: string
        default: ""
    environment:
      WORKSPACE: << parameters.tf_workspace >>
    working_directory: ~/project/environment
    steps:
      - checkout:
          path: ~/project
      - terraform/install
      - ecs_helper/install
      - run:
          name: Initialize
          command: terraform init
      - run:
          name: Set environment
          command: ~/project/.circleci/set_env.sh >> $BASH_ENV
      - run:
          name: Output
          command: terraform output -json > terraform.output.json
      - run:
          name: Run task
          command: |
            if [ "<< parameters.override >>" == "" ]
            then
              runner -task << parameters.task_name >> -timeout << parameters.timeout >>
            else
              runner -task << parameters.task_name >> -timeout << parameters.timeout >> -override << parameters.override >>
            fi
      - when:
          condition: << parameters.notify_slack >>
          steps:
            - slack/status:
                channel: opg-digideps-team
                failure_message: << parameters.task_name >> has failed.
                success_message: << parameters.task_name >> has succeeded.
